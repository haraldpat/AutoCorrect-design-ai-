{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoCorrect Tool\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motive: Whenever a word is typed we need to check the word and auto correct it if it's mispelled. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HOW?\n",
    "###### Approach 1\n",
    "We can make use of **\"Edit Distance\"**.\n",
    "\"Edit distance\" is a string metric, i.e. a way of quantifying how dissimilar two strings (e.g., words) are to one another, that is measured by counting the minimum number of operations required to transform one string into the other.\n",
    "We can create a group of correct words(dictionary) and when the test word finds the distances from all the words in dictionary and replaces it with the word having minimum distance. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make use of **Levenshtein distance**.\n",
    "Levenshtein distance between two words is the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate Levenshtein distance between two words\n",
    "def editDistance(s: str, t: str) -> int:\n",
    "    n = len(s)\n",
    "    m = len(t)\n",
    "\n",
    "    prev = [j for j in range(m+1)]\n",
    "    curr = [0] * (m+1)\n",
    "\n",
    "    for i in range(1, n+1):\n",
    "        curr[0] = i\n",
    "        for j in range(1, m+1):\n",
    "            if s[i-1] == t[j-1]:\n",
    "                curr[j] = prev[j-1]\n",
    "            else:\n",
    "                mn = min(1 + prev[j], 1 + curr[j-1])\n",
    "                curr[j] = min(mn, 1 + prev[j-1])\n",
    "        prev = curr.copy()\n",
    "\n",
    "    return prev[m]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list calculating the word's distance from every other words in the dictionary\n",
    "def find_word(diction: list,word: str) -> list:\n",
    "    final=[]\n",
    "    for w in diction:\n",
    "        final.append(editDistance(w,word))\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Before: I am hapu to sve ths county from rage spleling\n",
      "Text After: I am happy to save this country from rage spelling \n"
     ]
    }
   ],
   "source": [
    "diction = [\"save\",\"happy\",\"I\",\"want\",\"realistic\",\"this\",\"intuitive\",\"from\",\"to\",\"am\",\"rage\",\"spell\",\"check\",\"fortune\",\"resonance\",\"spelling\",\"country\",\"Please\",\"is\",\"a\"]\n",
    "text = \"I am hapu to sve ths county from rage spleling\"\n",
    "text2=text.split(\" \");\n",
    "ans=\"\"\n",
    "for word in text2:\n",
    "    dist_word = find_word(diction,word)\n",
    "    fword = diction[dist_word.index(min(dist_word))]\n",
    "    ans = ans+fword+\" \"\n",
    "print(\"Text Before: \"+text)\n",
    "print(\"Text After: \"+ans)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BUt this approach has many limitations in real world:\n",
    "(1) **Takes much time and space**\n",
    "Time Complexity: O(len(word) x len(dictionary))\n",
    "Space Complexity: O(len(Dictionary))\n",
    "And dictionary needs to be huge to encompass all the english words.\n",
    "(2) **Many words may have same minimum distance and so it becomes to difficult to choose which word is optimal to choose**\n",
    "(3) **It also becomes difficult to choose words in which context is the sentence being written**\n",
    "There can be chance that the desired word was not one with the minimum Levenshtein distance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Approach 2\n",
    "Now the previous approach gave us some lead to find the close words which the user might want to type, but which word he/she wants, this can be done by seeing their previous words which has been written and make a probabilistic assumptions from that probability list. This is an excellent application of A.I. specifically learning from the past experiences."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving to more further implementation, we would see another sort of string comparing method known as jacard index, it measures the similarity between two words and this can be helpful as to predict the word here. A very useful library text distance has many of this string matching algorithm, thus we can make use of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textdistance in c:\\users\\hp\\documents\\bharat intern\\autocorrect.ai\\.venv\\lib\\site-packages (4.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2 -> 23.2.1\n",
      "[notice] To update, run: c:\\Users\\Hp\\Documents\\Bharat Intern\\AutoCorrect.ai\\.venv\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\hp\\documents\\bharat intern\\autocorrect.ai\\.venv\\lib\\site-packages (1.25.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2 -> 23.2.1\n",
      "[notice] To update, run: c:\\Users\\Hp\\Documents\\Bharat Intern\\AutoCorrect.ai\\.venv\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\hp\\documents\\bharat intern\\autocorrect.ai\\.venv\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\documents\\bharat intern\\autocorrect.ai\\.venv\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\documents\\bharat intern\\autocorrect.ai\\.venv\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\hp\\documents\\bharat intern\\autocorrect.ai\\.venv\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\hp\\documents\\bharat intern\\autocorrect.ai\\.venv\\lib\\site-packages (from pandas) (1.25.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\documents\\bharat intern\\autocorrect.ai\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2 -> 23.2.1\n",
      "[notice] To update, run: c:\\Users\\Hp\\Documents\\Bharat Intern\\AutoCorrect.ai\\.venv\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install textdistance\n",
    "%pip install numpy\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textdistance\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in the text:['in', 'the', 'realm', 'of', 'imagination', 'where', 'dreams', 'weave', 'through', 'the']\n",
      "Total Unique words are 17755.\n"
     ]
    }
   ],
   "source": [
    "words=[]\n",
    "with open('rtext.txt','r',encoding=\"utf8\") as f:\n",
    "    fd = f.read()\n",
    "    fd=fd.lower()\n",
    "    words = re.findall('\\w+',fd)\n",
    "# Dictionary(Vocabulary) of words present in text\n",
    "D = set(words)\n",
    "print(f\"Words in the text:{words[0:10]}\")\n",
    "print(f\"Total Unique words are {len(D)}.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 14822), ('of', 6805), ('and', 6545), ('a', 4850), ('to', 4731)]\n"
     ]
    }
   ],
   "source": [
    "word_freq = {}  \n",
    "word_freq = Counter(words)\n",
    "print(word_freq.most_common()[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = {}     \n",
    "Total = sum(word_freq.values())    \n",
    "for k in word_freq.keys():\n",
    "    probs[k] = word_freq[k]/Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_autocorrect(input_word):\n",
    "    input_word = input_word.lower()\n",
    "    if input_word in D:\n",
    "        return {}\n",
    "    else:\n",
    "        sim = [1-(textdistance.Jaccard(qval=2).distance(v,input_word)) for v in word_freq.keys()]\n",
    "        df = pd.DataFrame.from_dict(probs, orient='index').reset_index()\n",
    "        df = df.rename(columns={'index':'Word', 0:'Prob'})\n",
    "        df['Similarity'] = sim\n",
    "        output = df.sort_values(['Similarity', 'Prob'], ascending=False)\n",
    "        return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ot = my_autocorrect(\"se\")\n",
    "ot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Before: I am writting rong wrds, let's see if it can find the wright one.\n",
      "Text After: I am writing wrong words let see if it can find the right one \n"
     ]
    }
   ],
   "source": [
    "text=\"I am writting rong wrds, let's see if it can find the wright one.\"\n",
    "text2 = text.split(\" \")\n",
    "ans = \"\"\n",
    "for word in text2:\n",
    "    out = my_autocorrect(word)\n",
    "    if(len(out)==0):\n",
    "        ans=ans+word+\" \"\n",
    "    else:\n",
    "        ans = ans+out.iloc[0][\"Word\"]+\" \"\n",
    "print(\"Text Before: \"+text)\n",
    "print(\"Text After: \"+ans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
